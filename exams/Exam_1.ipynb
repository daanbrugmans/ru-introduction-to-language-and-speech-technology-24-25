{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272QdUl-C7w4"
      },
      "source": [
        "\n",
        "##### LET-REMA-LCEX19 Introduction to Language and Speech Technology Take-home exam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**Theme: Een Taal is geen Taal**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Welcome to the take-home exam of the first part of the course. The theme of this exam is **Een Taal is geen Taal**: Most people speak more than one language, yet our language technologies tend to struggle with multilingual input - especially if the language is not one of the few languages that most work in NLP is done on. The challenges and limitations of enabling human language technologies to equally support all languages in the world are plenty. How are words (and other elements) in different languages even represented in computers? What about other levels of representation, large and small? From the very basics to complex ML applications, how do Natural Language Processing (NLP) pipelines typically deal with different languages and why is it so hard to build systems that work well across multiple languages? What practices and solutions has the field come up with? What challenges can multilingual data cause in specific NLP applications, e.g. text classification or language modeling? What inequalities are build into such technologies? Feel free to pursue your personal interests within this theme. Possible directions to take this in are:\n",
        "\n",
        "- how to build a multilingual NLP pipeline that works equally well across diverse languages;\n",
        "- compare two languages and contempate differences, challenges, and linguistic issues that arise;\n",
        "- comparisons between different varieties, dialects or sociolects with NLP pipelines/applications in mind.      \n",
        "\n",
        "If you have a significantly different type of essay in mind that you want to write, please contact us via email and quickly check with us.\n",
        "\n",
        "###  Instructions\n",
        "\n",
        "This is an **individual** take-home exam. Please complete the exam on your own and write entirely in your own words.\n",
        "\n",
        "Think of this exam as an essay discussing the theme “Een Taal is geen Taal”. Your text should comparatively discuss various issues that relate to how (elements of) different human languages are represented in NLP pipelines. Your discussion should include (at least) the following type of issues:\n",
        "\n",
        "- How does character encoding work in different languages?\n",
        "- How does tokenizing work in different languages?\n",
        "- How does stemming/lemmatization work in different languages?\n",
        "- How does parsing work in different languages?\n",
        "- How does text classification work ( e.g. bag of words, Naive Bayes vs LLMs)\n",
        "- How does language modelling (e.g. ngrams, Markov assumption, (instruction-tuned) large language models)\n",
        "\n",
        "\n",
        "Write the exam in form of an Python notebook (.ipynb). The easiest way to get started is to make a copy of this very document here and start working below. The exam should contain both text cells (written in full sentences) and code cells (with pseudocode) to illustrate relevant concepts.\n",
        "\n",
        "Use pseudocode only for illustrative purposes. No data needed - you do not need to actually build a data processing pipeline! You could, for instance, use pseudo code to illustrate how a word can be tokenised in different ways or how words of different languages are encoded as unicode etc.\n",
        "\n",
        "No minimum number of code cells is specified - include as many code cells in your document as you see fit.\n",
        "\n",
        "### What is pseudocode?\n",
        "\n",
        "If you are not familiar with pseudocode, start with this [short video tutorial](https://www.youtube.com/watch?v=qfckDdsEIq8).\n",
        "Your pseudocode does not have to be of any particular format. Just writing out the steps of your pipeline in plain English (like shown in the video tutorial) will suffice. You can also write in actual code, e.g. Python, but you will not get extra credits for doing so.\n",
        "\n",
        "### Length\n",
        "\n",
        "Aim for a submission of around 2500 words (excluding this existing instruction text, the bibliography, and your pseudocode cells). Diverging from this aim by more than 20% will impact the grade.   \n",
        "\n",
        "The easiest way to check the length of your document is to copy the text into a word processor or to download the file as Python code (.py) and open that file in Microsoft Word to count the words.\n",
        "\n",
        "\n",
        "### Exam weight\n",
        "This exam is 50% of the overall course grade.\n",
        "\n",
        "### Grading\n",
        "\n",
        "The exam will be graded for:\n",
        "\n",
        "- **Completeness** [primary]: Does the submission cover all aspects of preprocessing and NLP applications that were covered in the course? Please stick to the theme \"From words to numbers\" but cover all relevant linguistic and technical problems, challenges, and solutions of how various elements of written natural human language are represented and used in language technology.\n",
        "\n",
        "- **Depth of description** [secondary]: Are all concepts and steps explained clearly, thoroughly and concisely? Are all steps of a typical language processing pipelines described in sufficient detail? Is (pseudo)code used well for illustration purposes?\n",
        "\n",
        "- **Presentation** [tertiary]: Is the submission well organized, formatted, and professionally presented using both text and code cells? Your submission should be readable like an essay. It should not only entirely be written in full sentences (except the pseudocode cells), but quality of writing matters: write in a clear, academic, and succinct style. Include references to the course text book (link [link text](https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf)) where relevant and include a bibliography. References to literature other than to (Jurafsky and Martin 2023, p.xx) are encouraged but not required.\n",
        "\n",
        "The minimum grade to pass the exam is 5.5.\n",
        "\n",
        "The use of text generators (e.g. ChatGPT) is **not allowed** and is treated as a form of plagiarism, we will test each submission for that.\n",
        "\n",
        "### Submission\n",
        "\n",
        "Submit the exam via Brightspace as an iPython notebook (.ipynb) file **by the deadline**.\n",
        "\n",
        "\n",
        "Good luck!\n",
        "\n",
        "Andreas Liesenfeld and Cristian Tejedor-García"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfxitI1BlkqM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Your name: Daan Brugmans\n",
        "\n",
        "Student number: S1080742\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ideas\n",
        "Character Encoding\n",
        "- Character encoding is relatively straightforward for alphabets, abjads, syllabaries, and logographies.\n",
        "- Abugidas are not as easy; how should we represent abugidas?\n",
        "  - Take example language(s)\n",
        "  - Encode all combinations? Straightforward, but inefficient\n",
        "  - Alternative: encode consonants and vowels in an abugida separately, but render them as one character.\n",
        "  - How does the Unicode Consortium do this?\n",
        "- Example: typing kanji on Japanese phones (users write katakana/hiragana and the phone infers kanji)\n",
        "\n",
        "Tokenization\n",
        "- Word tokenization\n",
        "  - Multi-word units in alphabets\n",
        "  - Characters in logographies (Chinese languages)\n",
        "- Sentence tokenization\n",
        "  - Languages without end-of-sentence markers (Thai)\n",
        "\n",
        "Stemming/Lemmatization\n",
        "- Defining the word and the stem\n",
        "- Example: roots and stems in Arabic\n",
        "\n",
        "Parsing\n",
        "- Syntactic parsing\n",
        "  - Word order\n",
        "  - Case markings for indicating subject, objects, and the like\n",
        "  - In general: the linguistic typology\n",
        "- Semantic parsing\n",
        "  - One meaning can represented with very different phones in many languages (girl, meisje, famke, dèrnje for closely related languages), and vice versa\n",
        "  - Learning a semantic space is a solution for representing semantics across languages\n",
        "  - What about sentences with the same meaning, but with very different syntax (\"Hoe heet u\" vs. \"Van wie bu' gèj d'r een?\")?\n",
        "\n",
        "Text Classification\n",
        "- Since text classification is directly influenced by the contents of a text, it is inherently subject to the language(s) present in the unclassified texts\n",
        "- A multilingual text classification system must either be capable of handling any of the potential languages present in a text indiscriminately, or have separate models or systems for every potential language and apply the right one, the latter requiring language identification techniques.\n",
        "- Preferably, we would choose the former, but that requires that our singular system should be capable of handling any number of languages in theory, let alone in practice\n",
        "- Would machine translation offer a solution? Likely not, as syntactic and semantic intricacies of distant languages are often lost when translated.\n",
        "- Then how do we build a system that can accept any language as input, yet give the same result for the same task?\n",
        "  - Semantic spaces?\n",
        "\n",
        "Language Modelling\n",
        "- In LLMs, the tokens a model must process depend on the language it tries to model, and what these tokens represent also change by the type of script\n",
        "- Although with text classification, a system must be capable of accepting any potential language as input, in language modelling, the model must also produce an output of any potential language\n",
        "  - This could, and currently is theorized to be, achieved by learning a semantic space.\n",
        "- Can a language model be capable of code switching? If so, does it know that it is using multiple languages in the same produced text, or does it think it is only using one language?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9-pDrE9-_bh"
      },
      "source": [
        "# Introduction: state of the art, challenges and opportunities\n",
        "\n",
        "The concept of \"Eén Taal is Géén Taal\" (\"One Language is No Language\") is currently a hot topic within the field of NLP. As professional international communication is typically performed in English, which can be considered the modern *lingua franca*, it may come as no surprise that state-of-the-art language technology research is typically performed in and in service of English. The vast wealth of English language resources offers researchers plenty of data to train their models. English is what is (generally) considered a ***high-resource language***: a language with many resources from which plenty of data can be extracted for whatever language technology purpose. English may be considered one of the \"lucky few\" in the high-resource language group, as the ***low-resource language*** group, for which only a relatively small, limited number of resources exist, encompass most of the languages spoken and written. The low-resource language group not only emcompasses many minority languages that are recognized by governments, such as Papiamento and Frisian ([*Erkende talen in Nederland*, 2024](https://www.rijksoverheid.nl/onderwerpen/erkende-talen/erkende-talen-in-nl)), but also languages that aren't officially recognized, such as Brabantian and Zeelandic, local dialects, such as Huissens ([*Historische kring Huessen - Huussese Taol, 2024*](https://www.huessen.nl/over-huissen/dialect)), and even constructed languages, such as Esperanto. \n",
        "\n",
        "The current state-of-the-art in NLP is mostly based on high-resource languages, which is likely due to the deep learning models that underpin state-of-the-art technologies, which require vast amounts of data in order to be trained properly. Such models cannot be trained from scratch on low-resource languages, as the data that facilitates state-of-the-art models simply does not exist for those languages. As a consequence, most languages that are not considered high-resource cannot be processed by state-of-the-art models, despite the pool of high-resource languages being a very limited one. This is one major reason why multilingual NLP is such a hot topic: most languages are not even supported by modern tools yet, and the fact that those languages lack a wealth of data that can be used to implement such support is a major hurdle in realizing multilingual NLP systems.\n",
        "\n",
        "Within research on multilingual language models, one potential solution to this problem, if only a partial one, is to use existing language models trained on high-resource languages, and apply their knowledge on low-resource languages ([*Joshi. et al., 2024, Fine Tuning LLMs for Low Resource Languages*](https://doi.org/10.1109/icipcn63822.2024.00090)). Large Language Models pretrained on vast quantities of high-resource language data showcase a remarkable capability of modelling and producing language. These pretrained LLMs could then be finetuned to low-resource languages. The idea here is that the pretrained LLM has already acquired much knowledge about language in general when pretraining on the high-resource language(s), such as developing a semantic space in which it has learned to represent meanings in language as vectors, how to produce coherent and syntactically correct sentences, and, when Reinforcement Learning with Human Feedback is used, a sense of how to produce texts that cater to the needs of human agents. This knowledge is on top of the language-specific knowledge it has learned, such as the words used in the English language. By then finetuning an LLM on low-resource languages, the expectation is that the \"general language skills\" it has learned can be carried over onto the low-resource language, and that it does not need to relearn these general skills, allowing it to fully focus on learning only the skills that are specific to the low-resource language, such as its vocabulary and syntax. Ideally, the expectation that the finetuning LLM only has to learn the low-resource language itself, without improving its general language skills, means that it may need (much) less data to achieve state-of-the-art language modelling and generation for the low-resource language. In practice, that is easier said than done.\n",
        "\n",
        "Of course, this only describes one potential solution for building multilingual language modelling systems. But a multilingual NLP pipeline has many other problems to consider as well. I will discuss some of these problems in the rest of this essay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWFhZ4wz_OoB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80P5xLIx_xxq"
      },
      "source": [
        "# Elements of a maximally multilingual NLP pipeline\n",
        "\n",
        "In order to produce an NLP pipeline that is maximally multilingual, a key element of the pipeline should be the sheer variety that language offers. Language, both spoken and written, is represented in a great number of ways (for this essay, I will constrain myself to the topic of written language). A maximally multilingual pipeline must not only be capable of accepting language input of as many varying representations as possible, such as different types of scripts and as many languages as possible that use those scripts, but it must also be able to process and normalize all language input according to the language's syntax and semantics; normalizing language data in a way that does not work for that language results in garbage, and garbage in is garbage out. The downstream tasks that use the maximally multilingual NLP pipeline should, preferably, be equally multilingual as it, and so the models underpinning the downstream tasks must be capable of handling as many languages as possible. Succinctly: a maximally multilingual NLP pipeline must be *maximally flexible* in its language input.\n",
        "\n",
        "I will explore some elements from the inexhaustible list of qualities that make an NLP pipeline maximally multilingual. I divide this exploration in twain: one section about the handling, processing, and normalizing of maximally multilingual text data (\"Preprocessing\"), and the downstream tasks that make use of the maximally multilingual NLP pipeline (\"NLP applications\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cobuQCDlAAMk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcOoJlYuC786"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "Any text input is encoded by ***characters***. The *de facto* standard for character encoding nowadays is UTF-8, which represents characters by mapping one or multiple strings of 8 bits in a byte to a specific character used in a language, and is maintained by the Unicode Consortium. A maximally multilingual pipeline should use an existing encoding standard such as UTF-8 so that it can support as many character encodings as possible.\n",
        "\n",
        "For most scripts, character encoding is relatively straightforward. In alphabets, such as the Latin Alphabet for English and Dutch, or the Cyrillic Alphabet for Ukrainian and Bulgarian, both consonants and vowels get their own unique glyph, which can all get their own encoding. In Abjads, such as the Arabic and Hebrew scripts, only the consonants of words are written, and since every consonant also gets a glyph, they can be encoded relatively straightforwardly too. Syllabaries, such as the Japanese hiragana and katakana scripts and the Cherokee script, have a glyph for every syllable that is used in the language. Generally speaking, languages that use syllabaries as their script have simple syllable structures, such as [CV] (\"Consonant + Vowel\"), limiting the number of syllables, and thus glyphs, that exist in the script. Syllabary glyphs can thus also be relatively straightforwardly encoded.\n",
        "\n",
        "In logographies, such as the Chinese script or the related Japanese kanji script, a glyph represents a semantic component of the language. Since language is so expressive and could represent a practically inexhaustible number of semantics, logographies tend to be very large; knowing a few thousand unique characters with their meaning is considered basic when writing kanji, for example. Because of this, although encoding logographies is straightforward, it is also very costly, as many thousands of characters must all get their own unique encoding. The Unicode Consortium offers encodings for the Chinese script and kanji in a unified fashion, as may be seen in [*CJK Unified Ideographs (2024)*](https://unicode.org/charts/PDF/U4E00.pdf), a document which showcases all kanji and Chinese script glyphs encoded by the Unicode Standard, resulting in a document of more than 550 pages. \n",
        "\n",
        "Although we have encountered some issues, character encoding seems to be relatively straightforward for the scripts we discussed: each glyph used in a script can be directly mapped to an encoding, whether that glyph represents a consonant, vowel, syllable, or semantic component. However, when encountering and processing input from abugidas, we run into new issues. Abugidas are scripts where each glyph represents both a consonant *and* a vowel, just like a syllabary. However, in an abugida, the consonant is taken as the root of the glyph, and the vowel is appended to it, similar to a diacritic. Examples of abugidas are Devanagari (used in e.g. Hindi), Ge'ez (used in e.g. Amharic), and Canadian Aboriginal Syllabics (used in e.g. Inuktitut). As an example of how abugida glyphs are constructured, the following image by [*Saurmandal (2023)*](https://commons.wikimedia.org/wiki/File:Devanagari_matras.svg) displays varying Devanagari glyphs with the appended vowel markings highlighted on the consonant root of the glyph.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/d/d3/Devanagari_matras.svg)\n",
        "\n",
        "If we want to encode Devanagari, it would be straightforward to encode all possible consonant-vowel combinations separately, but this would also be very resource inefficient. Instead, we could take an approach where we encode the consonants and vowels separately, but *render* them as one glyph in downstream tasks. The latter is the approach taken by the Unicode Consortium ([*(Devanagari, 2024)*](https://unicode.org/charts/PDF/U0900.pdf)). This efficient encoding also has consequences for our pipeline and downstream tasks, as our maximally multilingual pipeline, and the (deep learning) models that use it, must be capable of recognizing that in abugida-based text input, a consonant encoding followed by a vowel encoding should be processed as if they were a singular glyph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coN_j2nNHjA_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpuPEbpDGdZ4"
      },
      "source": [
        "# NLP applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LydnauQ3l8l9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHWLMYS9_XwQ"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0h4N9PM_aI1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bibliography\n",
        "\n",
        "CJK Unified Ideographs: 2024. https://unicode.org/charts/PDF/U4E00.pdf. \n",
        "\n",
        "Devanagari: 2024. https://unicode.org/charts/PDF/U0900.pdf. \n",
        "\n",
        "Erkende talen in Nederland: 2024. https://www.rijksoverheid.nl/onderwerpen/erkende-talen/erkende-talen-in-nl. \n",
        "\n",
        "Historische kring Huessen - Huussese Taol: https://www.huessen.nl/over-huissen/dialect. \n",
        "\n",
        "Joshi, S. et al. 2024. Fine Tuning LLMs for Low Resource Languages. 5th International Conference on Image Processing and Capsule Networks (ICIPCN). (Jul. 2024), 511–519. DOI:https://doi.org/10.1109/icipcn63822.2024.00090. \n",
        "\n",
        "Saurmandal 2023. Devanagari matras."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
