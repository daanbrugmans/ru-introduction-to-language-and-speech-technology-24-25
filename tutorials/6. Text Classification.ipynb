{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSoy2gZNpqF3"
      },
      "source": [
        "# Part 1: Introduction to text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNzCiJPFKyk2"
      },
      "source": [
        "## 1. Getting started with Naive Bayes Text Classification\n",
        "\n",
        "#### Probabilistic Model of Classification\n",
        "\n",
        "In a probabilistic classification model we want to estimate the value of\n",
        "$P(c|x)$\n",
        ", the probability of a sample x being of class c. Naive Bayes is one such probabilistic classifier that uses Bayes' Rule to classify samples. And Naive Bayes is _\"Naive\"_ because it assumes strong independence among all the features of sample x.\n",
        "\n",
        "#### Bayes Rule:\n",
        "\n",
        "$P(c|x) = \\frac{P(x|c)P(c)}{P(x)}$\n",
        "\n",
        "#### Text Classification using Naive Bayes classifier\n",
        "\n",
        "Consider the task of classifying textual documents into having positive or negative sentiments. We will design the Naive Bayes classifier for this problem as follows:\n",
        "\n",
        "Samples are text documents, and their features are the words that comprises these documents.\n",
        "\n",
        "- Each document $d$ is a sequence of words, $d = w_1w_2...w_n$, where $w_i$ are the tokens of the document and $n$ is the total number of tokens in the document $d$.\n",
        "\n",
        "- The training dataset consists of many document, sentiment pairs, ${d_i, s_i}$\n",
        "\n",
        "- Each document $d_i$ is associated with a sentiment $s_i \\in \\{0,1\\}$, $0$ being negative sentiment and $1$ being positive sentiment.\n",
        "\n",
        "Using **Bayes' Rule** we have\n",
        "\n",
        "$p(s|d) = \\frac{p(d|s)p(s)}{p(d|s)p(s) + p(d|\\bar{s})p(\\bar{s})}$\n",
        "\n",
        "And from the **independence assumption** of features\n",
        "\n",
        "$p(d|s) = p(w_1,w_2,..., w_n|s) = p(w_1|s)p(w_2|s)...p(w_n|s)$\n",
        "\n",
        "Also in the **IMDb reviews dataset** that we are considering here have equal number of positive and negative datasets.\n",
        "\n",
        "We have $p(s) = 0.5$ and $p(\\bar{s})=0.5$.\n",
        "\n",
        "This simplifies our formulation for\n",
        "$p(s|d)$\n",
        "\n",
        "$ p(s|d) = \\frac{p(d|s)}{p(d|s) + p(d|\\bar{s})} $\n",
        "\n",
        "If we assign threshold of\n",
        "$p_T(s|d) = 0.5$\n",
        "for deciding the final label, the model simplifies to,\n",
        "\n",
        "$y=\n",
        "    \\begin{cases}\n",
        "      1, & \\text{if } p(d|s=1) \\geq p(d|s=0)\\\\\n",
        "      0, & \\text{otherwise}\n",
        "    \\end{cases}$\n",
        "#### A measure for numerical stability\n",
        "\n",
        "$p(w_i)$ will be very small in magnitude, and when we take a product of such very small numbers to compute $p(d|s)$\n",
        ", even double precision floating points fail to store such small numbers and becomes zero. Hence, for numerical stability, we will convert the probabilities to log probability,\n",
        "\n",
        "$\\log p(d|s) = \\log p(w_1,w_2,..., w_n|s) = \\log p(w_1|s) + \\log p(w_2|s) + ...+ \\log p(w_n|s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9hQ4GWbKykd"
      },
      "source": [
        "More on Naive Bayes for Sentiment analysis: Github Repository: [bsantraigi/Sentiment Analysis](https://github.com/bsantraigi/Sentiment-Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pgc5tG_TKykg"
      },
      "outputs": [],
      "source": [
        "# Import 'os' for preliminary tasks like directory listing etc.\n",
        "import os\n",
        "\n",
        "# Import re for regex string matching\n",
        "import re\n",
        "\n",
        "# Import nltk for word tokenization\n",
        "import nltk\n",
        "\n",
        "# Import Python's native data structures Counter and defaultdict\n",
        "# Counter - maintains count of element\n",
        "# defaultdict - dictionary data structure with exception handling for missing keys\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Import tqdm for progressbars\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "# Import numpy for different mathematical operations on arrays / matrices\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqYEfPkiNpvc",
        "outputId": "9ab34990-092a-4365-9a6a-9f4b5cde5e0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Daan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install the nltk punkt component for tokenization\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rskn91gKykl"
      },
      "source": [
        "### Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0vFdDGbKykm",
        "outputId": "da8eb35e-bdb9-4a2f-9224-071843fc4c73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAxLFB7oKykp"
      },
      "source": [
        "Extract data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6m_dPDAoKykp",
        "outputId": "b893cfef-cb30-44ff-d7db-4932136b9336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 76 ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tar: Error opening archive: Failed to open 'data/aclImdb_v1.tar.gz'\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!tar -xzf data/aclImdb_v1.tar.gz -C data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrMJG2ZFKykt"
      },
      "source": [
        "### Data Samples\n",
        "- Dataset is split into two parts for training and testing\n",
        "- Positive and negative samples are organized in individual folders\n",
        "- Each sample document is stored in a .txt file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsqaGfEKyku"
      },
      "source": [
        "### Let's read in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SJtY9mw6Kykv"
      },
      "outputs": [],
      "source": [
        "data_folder = 'data/aclImdb/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mJOwy_iyKyky"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: 'data/aclImdb/train/pos'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rp \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/pos\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m train_positive \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rp, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrp\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m      3\u001b[0m rp \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/neg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m train_negative \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rp, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(rp)]\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data/aclImdb/train/pos'"
          ]
        }
      ],
      "source": [
        "rp = os.path.join(data_folder, 'train/pos')\n",
        "train_positive = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
        "rp = os.path.join(data_folder, 'train/neg')\n",
        "train_negative = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
        "\n",
        "rp = os.path.join(data_folder, 'test/pos')\n",
        "test_positive = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
        "rp = os.path.join(data_folder, 'test/neg')\n",
        "test_negative = [os.path.join(rp, f) for f in os.listdir(rp)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXyAkzlcKyk3"
      },
      "source": [
        "### Regex for cleaning html tags\n",
        "- Pattern <.*?> means \"anything within two angular brackets\". The qualifier *? denotes \"as few times as possible\". This makes sure we match only one html tag at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTY671XQKyk4"
      },
      "outputs": [],
      "source": [
        "re_html_cleaner = re.compile(r\"<.*?>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmq06cnLKyk7"
      },
      "source": [
        "#### Limit number of samples\n",
        "To quickly train a small model, consider setting n_train and n_test to some relatively small numbers e.g. `1000`. Set,\n",
        "`n_train = n_test = -1` to use all the samples available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kTiqMYYKyk8"
      },
      "outputs": [],
      "source": [
        "n_train = 25000\n",
        "n_test = 25000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdJ5ALiQKylA"
      },
      "source": [
        "### (Conditional) Unigram Counter\n",
        "- Calculates the distribution $p(w|s=1)$ and $p(w|s=0)$, empirically, from training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-NMnbQaKylB"
      },
      "outputs": [],
      "source": [
        "# Distribution of word tokens in positive samples\n",
        "positive_word_counts = Counter()\n",
        "\n",
        "for _fname in tqdm_notebook(train_positive[:n_train], desc=\"Crunching +ve samples: \"):\n",
        "    with open(_fname) as f:\n",
        "        text = f.read().strip()\n",
        "        text = re_html_cleaner.sub(\" \", text)\n",
        "        positive_word_counts += Counter(nltk.word_tokenize(text))\n",
        "\n",
        "# Distribution of word tokens in negative samples\n",
        "negative_word_counts = Counter()\n",
        "\n",
        "for _fname in tqdm_notebook(train_negative[:n_train], desc=\"Crunching -ve samples: \"):\n",
        "    with open(_fname) as f:\n",
        "        text = f.read().strip()\n",
        "        text = re_html_cleaner.sub(\" \", text)\n",
        "        negative_word_counts += Counter(nltk.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmlumN40mxUQ"
      },
      "outputs": [],
      "source": [
        "print('Top k frequent words from positive class:\\n\\n')\n",
        "for w, c in positive_word_counts.most_common(10):\n",
        "    print(f\"{w}\\t{c}\")\n",
        "\n",
        "print('\\n\\nTop k frequent words from negative class:\\n\\n')\n",
        "for w, c in negative_word_counts.most_common(10):\n",
        "    print(f\"{w}\\t{c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqkE8xzKylH"
      },
      "source": [
        "#### Unigram counts to probability distribution\n",
        "\n",
        "$p(w|s) = \\frac{N_{s,w}}{N_{s,*}} = \\frac{N_{s,w}}{\\sum_{w' \\in W}N_{s,w'}}$\n",
        "\n",
        "#### Additive Smoothing\n",
        "- Note that, if some token, $u$, unseen in training documents, occurrs in a test document, $p(doc_{test}|s)$ becomes $0$ as $N_{s,u}$ for that token is $0$.\n",
        "- We apply _Additive Smoothing_ to prevent probability from going to zero.\n",
        "\n",
        "$p(w|s) = \\frac{\\alpha + N_{s,w}}{\\sum_{w' \\in W}(\\alpha + N_{s,w'})} = \\frac{\\alpha + N_{s,w}}{\\alpha V + \\sum_{w' \\in W}N_{s,w'}}$\n",
        "\n",
        "where V is the total vocab size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwWDtXqdKylH"
      },
      "outputs": [],
      "source": [
        "len_corpus_pos = sum(positive_word_counts.values())\n",
        "len_corpus_neg = sum(negative_word_counts.values())\n",
        "V_pos = len(positive_word_counts)\n",
        "V_neg = len(negative_word_counts)\n",
        "alpha = 0.1\n",
        "log_p_vocab_pos = defaultdict(\n",
        "    lambda: np.log(alpha/len_corpus_pos),\n",
        "    {w:np.log((alpha + c)/(V_pos*alpha + len_corpus_pos)) for w,c in positive_word_counts.items()}\n",
        ")\n",
        "log_p_vocab_neg = defaultdict(\n",
        "    lambda: np.log(alpha/len_corpus_neg),\n",
        "    {w:np.log((alpha + c)/(V_neg*alpha + len_corpus_neg)) for w,c in negative_word_counts.items()}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iud_C9nKylK"
      },
      "outputs": [],
      "source": [
        "p_data_pos = len(train_positive)/(len(train_positive) + len(train_negative))\n",
        "print(f\"Prob. of +ve sentiment in our dataset: {p_data_pos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMqVNMaUKylN"
      },
      "source": [
        "#### get_prob_pos(doc)\n",
        "\n",
        "A function that accepts a document string as input, tokenizes it and computes the probability\n",
        "$p(d|s=1)$\n",
        "and\n",
        "$p(d|s=0)$\n",
        ". It returns 1 if\n",
        "$p(d|s=1) \\geq p(d|s=0)$\n",
        "otherwise 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQZ8i40BKylP"
      },
      "outputs": [],
      "source": [
        "def get_prob_pos(doc):\n",
        "    text = doc.strip()\n",
        "    text = re_html_cleaner.sub(\" \", text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    p_pos = 1\n",
        "    p_neg = 1\n",
        "    for token in tokens:\n",
        "        p_pos += log_p_vocab_pos[token]\n",
        "        p_neg += log_p_vocab_neg[token]\n",
        "\n",
        "    return 1.0*(p_pos >= p_neg) #/(p_pos+p_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKRyUJICKylS"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for _fname in tqdm_notebook(test_positive[:n_test], desc=\"Classifying test data: \"):\n",
        "    with open(_fname) as f:\n",
        "        results.append((1, get_prob_pos(f.read())))\n",
        "\n",
        "\n",
        "for _fname in tqdm_notebook(test_negative[:n_test], desc=\"Classifying test data: \"):\n",
        "    with open(_fname) as f:\n",
        "        results.append((0, get_prob_pos(f.read())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo2WqwChKylY"
      },
      "source": [
        "### Performance evaluation of our model\n",
        "\n",
        "**Accuracy:** Overall performance of our model, fraction of samples that were labelled correctly\n",
        "\n",
        "**Recall:** Out of all +ve data samples in test set, what fraction of it was labelled correctly\n",
        "\n",
        "**Precision:** How precise is the model? Out of all samples that were tagged +ve by the model, how many were actually positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrmPCo4WKylV"
      },
      "outputs": [],
      "source": [
        "true_pos = 0\n",
        "false_pos = 0\n",
        "true_neg = 0\n",
        "false_neg = 0\n",
        "for true_label, pred_label in results:\n",
        "    if true_label == 1 and pred_label == 1:\n",
        "        true_pos += 1\n",
        "    elif true_label == 1 and pred_label == 0:\n",
        "        false_neg += 1\n",
        "    elif true_label == 0 and pred_label == 1:\n",
        "        false_pos += 1\n",
        "    elif true_label == 0 and pred_label == 0:\n",
        "        true_neg += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JbxE8PMKylZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy: {(true_pos + true_neg)/(true_pos + true_neg + false_pos + false_neg):0.4F}\")\n",
        "print(f\"Recall: {(true_pos)/(true_pos + false_neg):0.4F}\")\n",
        "print(f\"Precision: {(true_pos)/(true_pos + false_pos):0.4F}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3v8m4hbtgAG"
      },
      "source": [
        "### Classification metrics using sklearn package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsq03J3Gt_Bg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcaT4_tkKylc"
      },
      "outputs": [],
      "source": [
        "results[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA6q7Qc8uT2l"
      },
      "outputs": [],
      "source": [
        "u, v = zip((1, 1), (1, 0), (0, 1), (0, 1))\n",
        "print(u)\n",
        "print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBTYv67auO4H"
      },
      "outputs": [],
      "source": [
        "# Use zip to collect the first and second elements of each tuple within results\n",
        "# array to two separate lists\n",
        "y_true, y_pred = zip(*results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-sS5M4atdlk"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy (using sklearn package):\", accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA9aZQfdus_E"
      },
      "outputs": [],
      "source": [
        "# ----------------\n",
        "# Confusion Matrix\n",
        "# ----------------\n",
        "#\n",
        "# ----------\n",
        "#  |  0 |  1 <- pred\n",
        "# ----------\n",
        "# 0| tn | fp\n",
        "# ----------\n",
        "# 1| fn | tp\n",
        "# ----------\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0FwT9OUsk23"
      },
      "source": [
        "### F1-score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKozp8Ub85uM"
      },
      "source": [
        "Formula:\n",
        "F-score = 2 * (precision * recall) / (precision + recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jey-19EL86tt"
      },
      "outputs": [],
      "source": [
        "# Compute the F1 score, also known as balanced F-score\n",
        "precision = (true_pos)/(true_pos + false_pos)\n",
        "recall = (true_pos)/(true_pos + false_neg)\n",
        "F1_score = 2*(precision*recall)/(precision+recall)\n",
        "print(\"F1 Score: \", F1_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw8FW7VauRIh"
      },
      "source": [
        "# Part 2: Understanding text classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8673f9db-35ab-4337-a0a6-37b2f23a4481"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.text import Text\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
        "from nltk.probability import FreqDist, MLEProbDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvJsySsUzxYD"
      },
      "outputs": [],
      "source": [
        "# Install and import scattertext and its dependency pytextrank\n",
        "!pip install scattertext\n",
        "!pip install pytextrank\n",
        "\n",
        "import scattertext as st\n",
        "import pytextrank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hhGGccxrzUF"
      },
      "outputs": [],
      "source": [
        "# Download a spacy language model\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Mp95hZ6G9d"
      },
      "source": [
        "### Format data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifx0hj10xyx9"
      },
      "outputs": [],
      "source": [
        "def create_dataframe_from_txt_files():\n",
        "    data = {'text': [], 'label': []}\n",
        "\n",
        "    # Define the folders to iterate over\n",
        "    folders = [\"data/aclImdb/test/pos/\", \"data/aclImdb/train/pos/\",\n",
        "               \"data/aclImdb/test/neg/\", \"data/aclImdb/train/neg/\"]\n",
        "\n",
        "    # Iterate over each folder\n",
        "    for folder in folders:\n",
        "        label = \"positive\" if \"pos\" in folder else \"negative\"\n",
        "\n",
        "        # Iterate over .txt files in the folder\n",
        "        for filename in os.listdir(folder):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "                    data['text'].append(text)\n",
        "                    data['label'].append(label)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Call the function to create the DataFrame\n",
        "df = create_dataframe_from_txt_files()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TGYfHMW5rpP"
      },
      "outputs": [],
      "source": [
        "# Shuffle and downsize\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df = df.head(5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHxl5HeRyww4"
      },
      "source": [
        "### Run scattertext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-2tDjun07u0"
      },
      "outputs": [],
      "source": [
        "from scattertext import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDi6e9QBlNV4"
      },
      "outputs": [],
      "source": [
        "# Select the spacy tokenizer: LM or whitespace?\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "#nlp = scattertext.WhitespaceNLP.whitespace_nlp\n",
        "\n",
        "# Build \"corpus\"\n",
        "st_corpus = st.CorpusFromPandas(df, category_col='label', text_col='text',nlp=nlp).build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAoxLSfjNZ5f"
      },
      "outputs": [],
      "source": [
        "# Create a term frequency dataframe from the corpus\n",
        "term_freq_df = st_corpus.get_term_freq_df()\n",
        "# Most freq positive sentiment\n",
        "term_freq_df['Positive sentiment'] = st_corpus.get_scaled_f_scores('positive')\n",
        "# Most freq negative sentiment\n",
        "term_freq_df['Negative sentiment'] = st_corpus.get_scaled_f_scores('negative')\n",
        "\n",
        "# Print to distinguishing (by scaled F Score) and save to list\n",
        "print(\"Top positive: \\n\")\n",
        "print(list(term_freq_df.sort_values(by='Positive sentiment', ascending=False).index[:10]))\n",
        "print(\"\\nTop negative: \\n\")\n",
        "print(list(term_freq_df.sort_values(by='Negative sentiment', ascending=False).index[:10]))\n",
        "\n",
        "# Sort by top scores\n",
        "top_pos=term_freq_df.sort_values(by='Positive sentiment', ascending=False)\n",
        "top_neg=term_freq_df.sort_values(by='Negative sentiment', ascending=False)\n",
        "\n",
        "print(top_pos)\n",
        "print(top_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssc0Fg9BNgaP"
      },
      "outputs": [],
      "source": [
        "# Plot using scattertext explorer\n",
        "html = st.produce_scattertext_explorer(\n",
        "                   st_corpus,\n",
        "                   category='positive',\n",
        "                   category_name='Positive',\n",
        "                   not_category_name='Negative',\n",
        "                   width_in_pixels=1200,\n",
        "                   height_in_pixels=600,\n",
        "                   show_diagonal=True,\n",
        "                   show_characteristic=False,\n",
        "                   minimum_term_frequency=10,\n",
        "                   #d3_color_scale='d3.interpolateViridis',\n",
        "                   term_scorer=st.ScaledFScorePresets()) #or use term_scorer=st.RankDifference()\n",
        "                   #transform=st.Scalers.dense_rank)\n",
        "\n",
        "\n",
        "# Save html file\n",
        "html_file_name = \"explore_sentiment_analysis.html\"\n",
        "open(html_file_name, 'wb').write(html.encode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQlsnmeaAUZh"
      },
      "source": [
        "### Download results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R79zAS6U7aMS"
      },
      "outputs": [],
      "source": [
        "# Download the html file to open in browser\n",
        "\n",
        "from google.colab import files\n",
        "files.download('explore_sentiment_analysis.html')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
